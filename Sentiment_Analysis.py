# -*- coding: utf-8 -*-
"""Task4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tbTqnNERMl2pnfoKCSn7mNqQZKtghLFJ
"""

!pip install pandas matplotlib seaborn wordcloud scikit-learn

import pandas as pd

# Load CSV (no headers in original file)
df = pd.read_csv("twitter_training.csv", header=None)
df.columns = ["ID", "Entity", "Sentiment", "Tweet"]

df.head()

df.info()
df["Sentiment"].value_counts()
df["Entity"].value_counts().head(10)

def clean_tweet(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"@\w+|#\w+", "", text)
    text = re.sub(r"[^A-Za-z\s]", "", text)
    return text.lower().strip()

# Apply the function safely
df["Clean_Tweet"] = df["Tweet"].apply(clean_tweet)

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Generate word clouds for each sentiment
for sentiment in df["Sentiment"].unique():
    text = " ".join(df[df["Sentiment"] == sentiment]["Clean_Tweet"])
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"Word Cloud for {sentiment} Tweets")
    plt.show()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Vectorize the cleaned tweets
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df["Clean_Tweet"])
y = df["Sentiment"]

# Split and train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluate
print(classification_report(y_test, y_pred))